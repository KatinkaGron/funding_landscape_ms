{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6de2d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import deepl\n",
    "import random\n",
    "import re as re\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import translators as ts\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker \n",
    "# check \n",
    "%matplotlib inline\n",
    "import numbers\n",
    "import string\n",
    "from langdetect import detect\n",
    "from tabulate import tabulate\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pandas.errors import ParserError\n",
    "from langdetect import DetectorFactory\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder #maybe dont' need\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report, hamming_loss, accuracy_score, balanced_accuracy_score, make_scorer, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "#part 1 - dataset processing \n",
    "\n",
    "\n",
    "\n",
    "#functions adapted from: https://www.kaggle.com/code/abdmental01/text-preprocessing-nlp-steps-to-process-text\n",
    "\n",
    "def removing_html_tags(text):\n",
    "    \"\"\"\n",
    "    This function removes HTML tags in a given string (if it has one)\n",
    "    \n",
    "    input : text string\n",
    "    return : modified text without HTML tag\n",
    "    \n",
    "    \"\"\"\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', str(text))\n",
    "\n",
    "def removing_url(text):\n",
    "    \"\"\"\n",
    "    This function removes URLs in a given string (if it has one)\n",
    "    \n",
    "    input : text string\n",
    "    return : modified text without URL\n",
    "    \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "punc = string.punctuation  #initiating string punctuation variable\n",
    "\n",
    "def removing_string_punc(text):\n",
    "    \"\"\"\n",
    "    This function removes string punctuations (!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~)\n",
    "    \n",
    "    input : text string\n",
    "    return : modified text without string punctuations\n",
    "    \n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans('', '', punc))\n",
    "\n",
    "stop_words = get_stop_words('en') #initiating variable with english stop words\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"This function removes stop words from english vocabulary in a given string input (text)\"\"\"\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stop_words:\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "\n",
    "def data_processing_function(list_of_interest):\n",
    "    \"\"\"This function takes a list or series as input, and applies each datapre-processing steps defined before,\n",
    "    and return the processed list\"\"\"\n",
    "    \n",
    "    nl = list_of_interest.dropna().astype(str)\n",
    "    nl = nl.str.lower()  #Lowercase formatting\n",
    "    nl_html = nl.apply(removing_html_tags) #removing HTMLs\n",
    "    nl_url = nl_html.apply(removing_url) #removing URLs\n",
    "    nl_sp = nl_url.apply(removing_string_punc) #removing string punctuation: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    nl_stopword = nl_sp.apply(remove_stopwords) #removing stop words\n",
    "    return nl_stopword\n",
    "\n",
    "\n",
    "def find_category(category_col, cat_dictionary):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function will look for each string in the dictionary coining categories and subcategories to each class name,\n",
    "    and returns the correct sentiment  \n",
    "    \n",
    "    \"\"\"\n",
    "    new_sentinent = []\n",
    "    for string in category_col:\n",
    "        found = False\n",
    "        for key, values in cat_dictionary.items():\n",
    "            if string in values:\n",
    "                new_sentinent.append(key)\n",
    "                found = True\n",
    "                break  \n",
    "        if not found:\n",
    "            new_sentinent.append(\"NA\")\n",
    "            \n",
    "    return new_sentinent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#part 2 - dataset distribution and feature analysis\n",
    "\n",
    "def tf_idf_fun(df_column):\n",
    "    \"\"\" this function give a takes a dataframe with text samples as input, \n",
    "    it vectorizes the words into CountVectorizer() matrix, and then converts that into TF-IDF matrix\n",
    "    \n",
    "    return:\n",
    "    1. Vector with each feature (words) and its TF-IDF value\n",
    "    2. TF-IDF vector\n",
    "    3. CV vector\n",
    "    \n",
    "    \"\"\"\n",
    "    df_column = df_column.dropna()\n",
    "    cv=CountVectorizer()\n",
    "    cv_vec=cv.fit_transform(df_column) #transformation\n",
    "    print(cv_vec.shape)\n",
    "    \n",
    "    cv_df = pd.DataFrame(data= cv_vec.toarray(), columns = cv.get_feature_names_out())\n",
    "    \n",
    "    tf_transform=TfidfTransformer(smooth_idf=True,use_idf=True) #tfidf instansiating\n",
    "    tfidfvec = tf_transform.fit(cv_vec) #Transforming a count matrix into a tf-idf format\n",
    "    idf_data = pd.DataFrame(tf_transform.idf_, index=cv.get_feature_names_out(),columns=[\"tfidf_values\"]) #computing the IDF values\n",
    "    result_tfidf = idf_data.sort_values(by=['tfidf_values']) #sorting by idf value \n",
    "    return cv_vec, tfidfvec, result_tfidf\n",
    "\n",
    "def make_distribution(labeled1_df, name):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function makes a label distribution bar plot, illustrating the classes given in column \"sentiment\"\n",
    "    \"\"\"\n",
    "    output = labeled1_df[\"sentiment\"].value_counts().sort_index()  #checking dataset distribution\n",
    "    print(output)\n",
    "    plt.bar(output.index, output.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f\" {name} - Data Distribution\")\n",
    "    plt.savefig(f\"distributions_{name}.jpg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def make_distribution2(grouped_function, text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function makes a label distribution bar plot\n",
    "    \"\"\"\n",
    "    output = grouped_function\n",
    "    plt.bar(output.index, output.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f\" {text} - Category Data Distribution\")\n",
    "    plt.savefig(f\"distributions_prediction{text}.jpg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def ca_class_weights(df_annotation):\n",
    "    \"\"\"\n",
    "    This function returns a dictionary of class weights for each class, defined the following way:\n",
    "    from class_n0 .. class_n\n",
    "    \n",
    "    weight = total_samples / (n_total * class_n_samples)\n",
    "    \n",
    "    adapted from :\n",
    "    https://medium.com/@ravi.abhinav4/improving-class-imbalance-with-class-weights-in-machine-learning-af072fdd4aa4\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    unique_classes, class_counts = np.unique(df_annotation, return_counts=True)\n",
    "    total_samples = len(df_annotation)\n",
    "    class_weights = {}\n",
    "\n",
    "    for class_label, class_count in zip(unique_classes, class_counts):\n",
    "        class_weight = total_samples / (2.0 * class_count)\n",
    "        class_weights[class_label] = class_weight\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "\n",
    "#Part 3  --- prediction\n",
    "def conf_matrix_crossval(X, y, model, all_labels):\n",
    "    \"\"\"\n",
    "    confusion matrix w. k-fold cross-validation with 5 splits. \n",
    "    \n",
    "    \n",
    "    found inspiration through these two sources;\n",
    "    https://stackoverflow.com/questions/59282807/creating-k-dataframe-using-train-index-test-index-of-kfold-cross-validation-in\n",
    "    https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.KFold.html\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=37)\n",
    "    conf_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        y_test_enco = y_test\n",
    "        y_pred_enco = y_pred\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_enco, y_pred_enco, labels=range(len(all_labels)))\n",
    "        conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    return conf_matrices\n",
    "\n",
    "\n",
    "def fun_multilabel_test_train_data(df):\n",
    "\n",
    "    le = LabelBinarizer()\n",
    "    label_transformed = le.fit_transform(df[\"sentiment\"])\n",
    "    \n",
    "    text_data = df[\"merged_text\"]  # \n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_data, label_transformed, test_size=0.21, random_state=37)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, label_transformed, le\n",
    "\n",
    "def fun_multilabel_test_train_data2(df):\n",
    "\n",
    "    le = LabelBinarizer()\n",
    "    label_transformed = le.fit_transform(df[\"pred_merge\"])\n",
    "    \n",
    "    text_data = df[\"merged_text\"]  # \n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_data, label_transformed, test_size=0.21, random_state=37)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, label_transformed, le\n",
    "    \n",
    "\n",
    "def random_forest_hyperparameter_selection(x_training, x_testing, y_training, y_testing, lab):\n",
    "\n",
    "    \"\"\"\n",
    "    This function tests various random forest hyperparameters for optimization via grid search, \n",
    "    on the training and testing data of text data (x) and labels (y) : \n",
    "\n",
    "    n_estimators : Number of trees in the forest\n",
    "    max_depth: Max depth of the tree\n",
    "    min_samples_split: Minimum number of samples to split an internal node\n",
    "    min_samples_leaf: Minimum number of samples to be at a leaf node\n",
    "    source : https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "    \n",
    "\n",
    "    returns the best f1-score and lowest hamming loss derived from the best combination of hyperparameters\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    conf_matrix_rf = []\n",
    "    \n",
    "    parameter_grid = {'n_estimators': [50, 100, 150],'max_depth': [None, 10, 20],'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4]}\n",
    "    \n",
    "    random_forest = RandomForestClassifier(random_state=37, bootstrap=True)  #initiating random forest classifier with bootstrap\n",
    "\n",
    "    grid_search = GridSearchCV(random_forest, parameter_grid, cv=3, scoring='f1_weighted', n_jobs=-1) #initiating grid search with f1-scoring\n",
    "\n",
    "    \n",
    "    grid_search.fit(x_training, y_training)  #fitting\n",
    "    best_params = grid_search.best_params_  #fetch the best parameters from grid search\n",
    "    \n",
    "    optimized_random_forest = RandomForestClassifier(**best_params, random_state=37, bootstrap=True)  #calculating the results using the best parameters\n",
    "    optimized_random_forest.fit(x_training, y_training)  #fitting\n",
    "    predictions = optimized_random_forest.predict(x_testing)  #predicting the labels with x-dataset training data\n",
    "    \n",
    "    rflist = [(\"Random Forest\", optimized_random_forest)]\n",
    "    \n",
    "    y_training_enc = np.argmax(y_training, axis=1)\n",
    "    y_testing_enc = np.argmax(y_testing, axis=1)\n",
    "    \n",
    "    x_total = np.vstack([x_training.toarray(), x_testing.toarray()])\n",
    "    y_total = np.hstack([y_training_enc, y_testing_enc])\n",
    "    \n",
    "    \n",
    "    conf_res_rf =conf_matrix_crossval(x_total, y_total, optimized_random_forest, lab.classes_)\n",
    "    \n",
    "    RFmean_of_conf_matrix_arrays = np.mean(conf_res_rf, axis=0)\n",
    "    \n",
    "\n",
    "    scoring = {'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0),'accuracy': make_scorer(accuracy_score)}\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "    optimized_random_forest,\n",
    "    x_total,\n",
    "    y_total,\n",
    "    cv=5,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False)\n",
    "    \n",
    "    avg_cross_val_f1 = round(cv_results['test_f1_weighted'].mean(), 4)\n",
    "    avg_std_f1 = round(cv_results['test_f1_weighted'].std(), 4)\n",
    "    \n",
    "    avg_cross_val_BA = round(cv_results['test_accuracy'].mean(), 4)\n",
    "    avg_std_BA = round(cv_results['test_accuracy'].std(), 4)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print(optimized_random_forest)\n",
    "    print(\"Best hyperarameters from random forest classification optimization:\", best_params)\n",
    "    print(\"Weighted F1-score CV-avr:\", avg_cross_val_f1, \"std :\", avg_std_f1)\n",
    "    print(\"Accuracy CV-avr:\", avg_cross_val_BA, \"std :\", avg_std_BA)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_testing, predictions, target_names= lab.classes_))\n",
    "\n",
    "    return avg_cross_val_f1, avg_std_f1, avg_cross_val_BA, avg_std_BA, RFmean_of_conf_matrix_arrays, optimized_random_forest, rflist\n",
    "\n",
    "\n",
    "def parameter_est(xtrain, xtest, ytrain, ytest, binlab):\n",
    "    \"\"\"\n",
    "    This function tests various  hyperparameters for the models:\n",
    "    svm\n",
    "    Multinomial NB\n",
    "    Logistic Legression\n",
    "    \n",
    "    It optimizes using grid search, and takes the best hyperparameter to estimate f1-score and hamming loss.\n",
    "    \n",
    "    Input: training and testing data of text data (x), labels (y) and binary label format: \n",
    "\n",
    "    return: The best f1-score and lowest hamming loss for each model. \n",
    "    \"\"\"\n",
    "    \n",
    "    f1_score_list = []\n",
    "    f1_std_list = []\n",
    "    BA_list = []\n",
    "    BA_std_list = []\n",
    "    model_t = []\n",
    "    model_choice_list = []\n",
    "    conf_matrix_list = []\n",
    "    \n",
    "    models_passed = []\n",
    "    \n",
    "    \n",
    "    scoring = {'f1_weighted': make_scorer(f1_score, average='weighted'),'accuracy': make_scorer(accuracy_score)}\n",
    "    \n",
    "    \n",
    "    sup_vm = LinearSVC(penalty='l2', loss='squared_hinge')\n",
    "    multi_nb = MultinomialNB()\n",
    "    logreg = LogisticRegression(solver=\"liblinear\")\n",
    "    \n",
    "    #hyperparameter grids\n",
    "    param_grid_svm = {\"estimator__C\": [1, 0.5, 1.5, 2], \"estimator__max_iter\": [1000, 2000, 3000, 4000], \"estimator__class_weight\": [None, \"balanced\"]}\n",
    "    param_grid_NB = {\"estimator__alpha\": [1, 10**-3, 10**-1, 10**1], \"estimator__fit_prior\": [True, False]}\n",
    "    param_grid_logreg = {\"estimator__class_weight\": [None, \"balanced\"], \"estimator__max_iter\": [3000, 4000], \"estimator__C\":[1, 1.5, 2]}\n",
    "\n",
    "    model_selection = [param_grid_svm, param_grid_NB, param_grid_logreg]\n",
    "    model_call = [sup_vm, multi_nb, logreg]\n",
    "    modelname = [\"SVM\", \"Multinomial NB\", \"Logistic Reg\"]\n",
    "    \n",
    "    for num, model_choice in enumerate(model_call):\n",
    "        \n",
    "        ova = OneVsRestClassifier(model_choice)\n",
    "        \n",
    "        #grid search\n",
    "        grid_search = GridSearchCV(ova, model_selection[num], scoring='f1_weighted', cv=3, n_jobs=-1)\n",
    "        grid_search.fit(xtrain, ytrain)\n",
    "        best_params = grid_search.best_params_\n",
    "        \n",
    "        params_to_set = {}\n",
    "        for inner, va in best_params.items():\n",
    "            key = inner.split('__')[1]\n",
    "            params_to_set[key] = va\n",
    "\n",
    "        model_choice.set_params(**params_to_set)\n",
    "        \n",
    "        optimized_model = OneVsRestClassifier(model_choice)\n",
    "        optimized_model.fit(xtrain, ytrain)\n",
    "        \n",
    "        model_choice_list.append(optimized_model)\n",
    "        predictions = optimized_model.predict(xtest)\n",
    "        \n",
    "        models_passed.append((modelname[num], optimized_model))\n",
    "        #cv\n",
    "        \n",
    "        \n",
    "        ytrain_enc = np.argmax(ytrain, axis=1)\n",
    "        ytest_enc = np.argmax(ytest, axis=1)\n",
    "        \n",
    "        x_total = np.vstack([xtrain.toarray(), xtest.toarray()])\n",
    "        y_total = np.hstack([ytrain_enc, ytest_enc])\n",
    "        \n",
    "        \n",
    "        cv_results = cross_validate(\n",
    "        optimized_model,\n",
    "        x_total,\n",
    "        y_total,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False)\n",
    "\n",
    "        avg_cross_val_f1 = round(cv_results['test_f1_weighted'].mean(), 4)\n",
    "        avg_std_f1 = round(cv_results['test_f1_weighted'].std(), 4)\n",
    "\n",
    "        avg_cross_val_BA = round(cv_results['test_accuracy'].mean(), 4)\n",
    "        avg_std_BA = round(cv_results['test_accuracy'].std(), 4)\n",
    "\n",
    "        conf_res = conf_matrix_crossval(x_total, y_total, optimized_model, binlab.classes_)\n",
    "        mean_of_conf_matrix_arrays = np.mean(conf_res, axis=0)\n",
    "        \n",
    "        print(model_choice)\n",
    "        print(\"Best hyperarameters :\", best_params)\n",
    "        print(\"Weighted F1-score CV-avr:\", avg_cross_val_f1, \"std :\", avg_std_f1)\n",
    "        print(\"Accuracy CV-avr:\", avg_cross_val_BA, \"std :\", avg_std_BA)\n",
    "        print(\"Classification Report:\\n\", classification_report(ytest, predictions, target_names= binlab.classes_))\n",
    "        \n",
    "        f1_score_list.append(avg_cross_val_f1)\n",
    "        f1_std_list.append(avg_std_f1)\n",
    "        BA_list.append(avg_cross_val_BA)\n",
    "        BA_std_list.append(avg_std_BA)\n",
    "        model_t.append(modelname[num])\n",
    "        \n",
    "        conf_matrix_list.append(mean_of_conf_matrix_arrays)\n",
    "        \n",
    "    return f1_score_list, f1_std_list, BA_list, BA_std_list, conf_matrix_list, model_t, model_choice_list, models_passed\n",
    "\n",
    "def all_reports_cv(df, df_new_predictions, testname):\n",
    "    \"\"\"\n",
    "    Multiple label classification preperation by optimizing the \"random forest\" hyperparameters, by testing and scoring the different hyperparameters, \n",
    "    and choosing the parameters yielding the lowest hamming loss. \n",
    "    \n",
    "    A classification report, F1-score and a hamming loss is printed for 2 vectorization methods (TF-IDF and CV)\n",
    "    \n",
    "    3. hyperparameter selection testing using bootstrapping and grid search\n",
    "    \n",
    "    return: 2 tables, one for each vectorization method, their best hyperparameters and hamming loss of those. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    table_rf = []\n",
    "    table_f = []\n",
    "    \n",
    "    confusion_rf = []\n",
    "    confusion_rest = []\n",
    "    \n",
    "    model_choices_list = []\n",
    "    featstrat_list = []\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test, binary_label, le_e = fun_multilabel_test_train_data(df)\n",
    "\n",
    "    count_vect = CountVectorizer()  \n",
    "    tfidf_vect = TfidfVectorizer()  \n",
    "    \n",
    "    for vectorizer in [count_vect, tfidf_vect]:\n",
    "        xtrain_vec = vectorizer.fit_transform(x_train)\n",
    "        xtest_vec = vectorizer.transform(x_test)\n",
    "        feature_selection_method = str(vectorizer).split('(')[0]\n",
    "        print(\"feature selection strategy :\", feature_selection_method)\n",
    "        \n",
    "        RF_cv_f1, RF_cv_f1_std, RF_cv_BA, RF_cv_std, rf_cv_confmatrix, best_model, rf_est = random_forest_hyperparameter_selection(xtrain_vec, xtest_vec, y_train, y_test, le_e)\n",
    "\n",
    "        table_rf.append([\n",
    "            \"Random Forest\",\n",
    "            \"F1-score\",\n",
    "            feature_selection_method,\n",
    "            RF_cv_f1\n",
    "        ])\n",
    "        table_rf.append([\n",
    "            \"Random Forest\",\n",
    "            \"F1-score std.\",\n",
    "            feature_selection_method,\n",
    "            RF_cv_f1_std\n",
    "        ])\n",
    "        table_rf.append([\n",
    "            \"Random Forest\",\n",
    "            \"Accuracy\",\n",
    "            feature_selection_method,\n",
    "            RF_cv_BA\n",
    "        ])\n",
    "        table_rf.append([\n",
    "            \"Random Forest\",\n",
    "            \"Accuracy std.\",\n",
    "            feature_selection_method,\n",
    "            RF_cv_std\n",
    "        ])\n",
    "        \n",
    "        model_choices_list.append(best_model)\n",
    "        \n",
    "        featstrat_list.append(feature_selection_method)\n",
    "        \n",
    "        confusion_rf.append(rf_cv_confmatrix)\n",
    "        \n",
    "        LMS_f1_list, LMS_f1_std_list, LMS_BA_list, LMS_BA_std_list, LMS_conf_matrix_list, LMS_model_t, best_model, LMS_est = parameter_est(xtrain_vec, xtest_vec, y_train, y_test, le_e)\n",
    "        \n",
    "        \n",
    "        models_passed = rf_est + LMS_est\n",
    "        model_choices_list.append(best_model)\n",
    "        featstrat_list.append(feature_selection_method)\n",
    "        for num, i in enumerate(LMS_f1_list):\n",
    "\n",
    "            table_f.append([\n",
    "                LMS_model_t[num],\n",
    "                \"F1-score\",\n",
    "                feature_selection_method,\n",
    "                i\n",
    "            ])\n",
    "            \n",
    "            table_f.append([\n",
    "                LMS_model_t[num],\n",
    "                \"F1-score std\",\n",
    "                feature_selection_method,\n",
    "                LMS_f1_std_list[num]\n",
    "            ])\n",
    "            \n",
    "            table_f.append([\n",
    "                LMS_model_t[num],\n",
    "                \"Accuracy\",\n",
    "                feature_selection_method,\n",
    "                LMS_BA_list[num]\n",
    "            ])\n",
    "            table_f.append([\n",
    "                LMS_model_t[num],\n",
    "                \"Accuracy std.\",\n",
    "                feature_selection_method,\n",
    "                LMS_BA_std_list[num]\n",
    "            ])\n",
    "            \n",
    "            confusion_rest.append(LMS_conf_matrix_list[num])\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    df_new_pred, df_top = result_maker_v2(0, df, df_new_predictions, testname, table_f, table_rf, confusion_rf, confusion_rest, binary_label, le_e, model_choices_list, feature_selection_method, x_train, x_test, y_train, y_test, binary_label, le_e, models_passed)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_new_pred, df_top\n",
    "\n",
    "\n",
    "def heatmap_maker(conf_matrix_2d, label_columns, modelname, feat_strat):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function produces a heatmap from a confusion matrix and column of labels (in correct order)\n",
    "    \n",
    "    \"\"\"\n",
    "    ax = sns.heatmap(conf_matrix_2d, annot=True, fmt=\".1f\", cmap='YlGnBu', annot_kws={\"fontsize\":7})\n",
    "    \n",
    "    ax.set_xlabel(\"Predicted Label\", fontsize=14, labelpad=20)\n",
    "    ax.xaxis.set_ticklabels(label_columns, rotation=90)\n",
    "    ax.set_ylabel(\"True Label\", fontsize=14, labelpad=20)\n",
    "    ax.yaxis.set_ticklabels(label_columns, rotation=0)\n",
    "    ax.set_title(f\"Heatmap - ({modelname} and {feat_strat}) - Predicted vs. True\", fontsize=14, pad=20)\n",
    "    plt.show()\n",
    "    return\n",
    " \n",
    "\n",
    "                                                                                      \n",
    "def coefficient_printerv2(df, df_new, class_names, method_inst, model_name, choosen_vec_strategy, x_train, x_test, y_train, y_test, binary_label, le_e, models_passed):\n",
    "    \n",
    "    selected_model = None  \n",
    "    \n",
    "    for name, model in models_passed:\n",
    "        if name == model_name:\n",
    "            selected_model = model\n",
    "            print(f\"Selected model: {model_name}\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    str_vect = choosen_vec_strategy\n",
    "    xtrain_vec = str_vect.fit_transform(x_train)\n",
    "    xtest_vec = str_vect.transform(x_test)\n",
    "    \n",
    "\n",
    "    selected_model.fit(xtrain_vec, y_train)\n",
    "    \n",
    "\n",
    "    predictions_best = selected_model.predict(xtest_vec)\n",
    "    \n",
    "    list_of_pos_corr = []\n",
    "\n",
    "    for num, class_name in enumerate(class_names):\n",
    "        class_coefficients = selected_model.estimators_[num].coef_[0]\n",
    "\n",
    "        top_positive = np.argsort(class_coefficients)[-10:] \n",
    "        top_pos = []\n",
    "        for i in top_positive:\n",
    "            top_pos.append(str_vect.get_feature_names_out()[i])\n",
    "            \n",
    "        list_of_pos_corr.append(top_pos)\n",
    "\n",
    "\n",
    "    full_data_xtest = str_vect.transform(df_new[\"merged_text\"])\n",
    "    predictions = selected_model.predict(full_data_xtest)\n",
    "\n",
    "    new = le_e.inverse_transform(predictions)\n",
    "    df_new[\"prediction\"] = new\n",
    "    print(len(list_of_pos_corr))\n",
    "\n",
    "    return list_of_pos_corr, class_names, df_new\n",
    "\n",
    "\n",
    "def make_table(table_rest, table_random_forest, testname):\n",
    "    \"\"\"\n",
    "    This function, takes the tables created in the all_results function, and makes them into an easyly read format\n",
    "    using tabulate. \n",
    "    input:\n",
    "    table_random_forest = table output from random forest tests\n",
    "    table_rest = table output from 3 remaining tests \n",
    "    \n",
    "    return:\n",
    "    table = table_output\n",
    "    df_result_sort_out = dataframe with results sorted\n",
    "    df_results = dataframe with results nonsorted\n",
    "    \"\"\"\n",
    "    \n",
    "    comb_tables = (table_rest+table_random_forest)\n",
    "    df_results = pd.DataFrame(comb_tables, columns = [\"model_name\", \"score_method\", \"feat_strat\", \"score\"])\n",
    "    df_result_sorted = df_results.sort_values(by=['feat_strat', 'model_name'])\n",
    "    df_result_sort_out = df_result_sorted.loc[:,[\"feat_strat\", 'model_name','score_method','score']]\n",
    "    df_table = df_result_sorted.reset_index(drop = True)\n",
    "    headers = [\"Feature Selection Strategy\", \"Model\", \"Scoring Method\", \"Value\"]\n",
    "    table = tabulate(df_table, headers=headers, tablefmt=\"pretty\", colalign=(\"center\",) * len(headers), numalign=\"center\", maxcolwidths = 20)\n",
    "    print(f\" score overview test : {testname}\")\n",
    "    return table, df_result_sort_out, df_results\n",
    "\n",
    "\n",
    "def scatterplot_maker(df_sorted, name_string):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function creates a scatterplot out of the test results, plotting F1-score vs. inverse Hamming Loss\n",
    "    input:\n",
    "    df_sorted (from make_table function)\n",
    "    \n",
    "    return:\n",
    "    scatterplot \n",
    "    sorted_df = dataframe sorted by score \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #df_sorted = df_sorted[~((df_sorted[\"feat_strat\"] == \"TfidfVectorizer\") & (df_sorted[\"model_name\"] == \"Logistic Reg\"))]\n",
    "    f1_scores = df_sorted[df_sorted[\"score_method\"] == \"F1-score\"]\n",
    "    balanced_acc = df_sorted[df_sorted[\"score_method\"] == \"Accuracy\"]\n",
    "    \n",
    "    f1_scores = f1_scores.rename(columns={\"score\": \"score_f1\"})\n",
    "    balanced_acc_named = balanced_acc.rename(columns={\"score\": \"bal_accuracy\"})\n",
    "\n",
    "    mergeddf = pd.merge(\n",
    "        f1_scores[[\"feat_strat\", \"model_name\", \"score_f1\"]],\n",
    "        balanced_acc_named[[\"feat_strat\", \"model_name\", \"bal_accuracy\"]],\n",
    "        on=[\"feat_strat\", \"model_name\"])\n",
    "    \n",
    "    mergeddf[\"maxscore\"] = (mergeddf[\"score_f1\"] + (mergeddf[\"bal_accuracy\"]))\n",
    "    \n",
    "    sorted_df = mergeddf.sort_values(by=[\"maxscore\"], ascending=[False], ignore_index = True)\n",
    "\n",
    "    count_vect = df_sorted[df_sorted[\"feat_strat\"] == \"CountVectorizer\"]\n",
    "    tfidf_vect = df_sorted[df_sorted[\"feat_strat\"] == \"TfidfVectorizer\"]\n",
    "\n",
    "    CV_f1score, CV_hammingloss_mask = count_vect[\"score_method\"] == \"F1-score\", count_vect[\"score_method\"] == \"Accuracy\"\n",
    "    f1score_tfidf, hammingloss_tfidf = tfidf_vect[\"score_method\"] == \"F1-score\", tfidf_vect[\"score_method\"] == \"Accuracy\"\n",
    "    \n",
    "    \n",
    "    hamming_cv_yvalue = np.array(count_vect[CV_hammingloss_mask][\"score\"])\n",
    "    f1score_cv_xvalue = np.array(count_vect[CV_f1score][\"score\"])\n",
    "    print(hamming_cv_yvalue)\n",
    "    \n",
    "    CV_modelnames = count_vect[CV_f1score][\"model_name\"]\n",
    "    CV_modelnames = list(CV_modelnames)\n",
    "    \n",
    "    for num, score in enumerate(f1score_cv_xvalue):\n",
    "        plt.text(score, hamming_cv_yvalue[num], CV_modelnames[num], fontsize = 7)\n",
    "\n",
    "    \n",
    "    TF_modelname = tfidf_vect[f1score_tfidf][\"model_name\"]\n",
    "\n",
    "    f1score_tfidf_xvalue = np.array(tfidf_vect[f1score_tfidf][\"score\"])\n",
    "    hammingloss_tfidf_yvalue = np.array(tfidf_vect[hammingloss_tfidf][\"score\"])\n",
    "    \n",
    "\n",
    "    TF_modelname = list(TF_modelname)\n",
    "    for num, score in enumerate(f1score_tfidf_xvalue):\n",
    "        if TF_modelname[num] == \"Random Forest\":\n",
    "            y_adjust = 0.0013\n",
    "            plt.text(score, (hammingloss_tfidf_yvalue[num]-y_adjust), TF_modelname[num], fontsize = 7)\n",
    "        else:\n",
    "            plt.text(score,hammingloss_tfidf_yvalue[num], TF_modelname[num], fontsize = 7)\n",
    "\n",
    "    plt.arrow(sorted_df[\"score_f1\"][0] - 0.003, sorted_df[\"bal_accuracy\"][0] - 0.003, 0.0020, 0.0020, width = 0.0005, length_includes_head = True,\n",
    "              head_width=0.002, head_length=0.001, fc='green', ec='green')\n",
    "\n",
    "    plt.scatter(f1score_cv_xvalue, hamming_cv_yvalue, label = 'CountVectorizer', color = 'blue')\n",
    "    plt.scatter(f1score_tfidf_xvalue, hammingloss_tfidf_yvalue, label = 'TF-IDF Vectorizer', color = 'red')\n",
    "\n",
    "    plt.xlabel('F1-score')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('F1-score vs. Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout() \n",
    "    #plt.subplots_adjust(top=0.9, bottom=0.1)\n",
    "    plt.savefig(f\"scatterplot_{name_string}.jpg\", bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    return sorted_df\n",
    "\n",
    "def heatmap_printer(df_res, con_m_rf, con_m_rest, label_encode_i):\n",
    "    \n",
    "    \n",
    "    df_rf = df_res[df_res[\"model_name\"] == \"Random Forest\"]\n",
    "    df_rest = df_res[df_res[\"model_name\"] != \"Random Forest\"]\n",
    "\n",
    "    rf_mask = df_rf[\"score_method\"] == \"F1-score\"\n",
    "    fstrat_rf = list(df_rf[rf_mask][\"feat_strat\"])\n",
    "\n",
    "    rest_mask = df_rest[\"score_method\"] == \"F1-score\"\n",
    "\n",
    "    fstrat_rest = list(df_rest[rest_mask][\"feat_strat\"])\n",
    "    model_names = list(df_rest[rest_mask][\"model_name\"])\n",
    "\n",
    "    for num, i in enumerate(con_m_rf):\n",
    "        #[i] = i\n",
    "        heatmap_maker(i, label_encode_i.classes_, \"Random Forest\", fstrat_rf[num])\n",
    "\n",
    "\n",
    "    for num, i in enumerate(con_m_rest):\n",
    "        heatmap_maker(i, label_encode_i.classes_, model_names[num], fstrat_rest[num])\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "def result_maker_v2(row_dfsorted, df, df_new_predictions, testname, table_rest_out, table_random_forest_out, confusion_rf, confusion_rest, bin_label, label_encode, model_list, feat_strategy, x_train, x_test, y_train, y_test, binary_label, le_e, models_passed):\n",
    "\n",
    "    #table_rest_out, table_random_forest_out, confusion_rf, confusion_rest, bin_label, label_encode, model_list, feat_strategy  = all_reports(df)\n",
    "    \n",
    "    table_output, df_result_sorted, df_res = make_table(table_rest_out, table_random_forest_out, testname)\n",
    "    \n",
    "    print(table_output)\n",
    "    \n",
    "    scores_sorted = scatterplot_maker(df_result_sorted, testname)\n",
    "    scores_sorted = scores_sorted.sort_index()\n",
    "    print(\"this works\")\n",
    "    print(scores_sorted)\n",
    "    heatmap_printer(df_res, confusion_rf, confusion_rest, label_encode)\n",
    "    print(\"this works\")\n",
    "    feature_strategy_mapping = {\n",
    "        \"TfidfVectorizer\": TfidfVectorizer,\n",
    "        \"CountVectorizer\": CountVectorizer\n",
    "    }\n",
    "        \n",
    "    print(model_list)\n",
    "\n",
    "    model_strategy_mapping = {\n",
    "        \"TfidfVectorizer\": {\n",
    "            \"Logistic Reg\": model_list[3][2],\n",
    "            \"SGD\": model_list[3][0],\n",
    "            \"Multinomial NB\": model_list[3][1],\n",
    "            \"Random Forest\": model_list[2]\n",
    "        },\n",
    "        \"CountVectorizer\": {\n",
    "            \"Logistic Reg\": model_list[1][2],\n",
    "            \"SGD\": model_list[1][0],\n",
    "            \"Multinomial NB\": model_list[1][1],\n",
    "            \"Random Forest\": model_list[0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    top_feat_strat = scores_sorted[\"feat_strat\"][row_dfsorted]\n",
    "    top_model_name = scores_sorted[\"model_name\"][row_dfsorted]\n",
    "    \n",
    "\n",
    "    feat_strategy = feature_strategy_mapping.get(top_feat_strat, lambda: None)()\n",
    "    model_strategy = model_strategy_mapping.get(top_feat_strat, {}).get(top_model_name, None)\n",
    "\n",
    "\n",
    "    print(feat_strategy)\n",
    "    print(\"NAME, :\", top_model_name)\n",
    "    print(model_strategy)\n",
    "    \n",
    "\n",
    "                                                                                      \n",
    "    list_of_corr, classnames, df_new_pred_n1_sgd = coefficient_printerv2(df, df_new_predictions, label_encode.classes_, model_strategy, top_model_name, feat_strategy, x_train, x_test, y_train, y_test, binary_label, le_e, models_passed)\n",
    "    nl = list(list_of_corr)\n",
    "    df_top = pd.DataFrame(nl)\n",
    "    df_top = df_top.transpose()\n",
    "    df_top.columns = classnames\n",
    "    for num, i in enumerate(list_of_corr):\n",
    "        print(f\"Top positive correlations for {classnames[num]}:\")\n",
    "        print(i)\n",
    "        print(\"\\n\")\n",
    "    groupin = df_new_pred_n1_sgd.groupby([\"prediction\"])[\"Year\"].count()\n",
    "    print(df_new_pred_n1_sgd.groupby([\"prediction\"])[\"Year\"].count())\n",
    "    make_distribution2(groupin, \"predictions\")\n",
    "    \n",
    "    return df_new_pred_n1_sgd, df_top\n",
    "\n",
    "\n",
    "def foundations_asso_print(df, foundation_name):\n",
    "\n",
    "    df_mask = df[\"Foundation_name\"] == foundation_name\n",
    "\n",
    "    df_foundation_data = df[df_mask]\n",
    "    print(len(df_foundation_data))\n",
    "    x_train, x_test, y_train, y_test, label_transformed, le_group = fun_multilabel_test_train_data2(df_foundation_data)\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    xtrain_vec = tfidf_vect.fit_transform(x_train)\n",
    "    xtest_vec = tfidf_vect.transform(x_test)\n",
    "\n",
    "    sup_vm = LinearSVC(penalty='l2', loss='squared_hinge', C=1, class_weight='balanced', max_iter = 1000)\n",
    "\n",
    "    ova = OneVsRestClassifier(sup_vm)\n",
    "\n",
    "    ova.fit(xtrain_vec, y_train)\n",
    "\n",
    "    predictions = ova.predict(xtest_vec)\n",
    "\n",
    "    f1score = round(f1_score(y_test, predictions, average='weighted'), 4)\n",
    "\n",
    "    print(\"F1-score :\", f1score)\n",
    "    print(\"Classification Report :\",foundation_name)\n",
    "    class_report = classification_report(y_test, predictions, target_names= le_group.classes_, output_dict=True)\n",
    "    class_reportfll = classification_report(y_test, predictions, target_names= le_group.classes_)\n",
    "    \n",
    "    list_of_pos_correlations = []\n",
    "\n",
    "    for i, class_name in enumerate(le_group.classes_):\n",
    "        class_coefficients = ova.estimators_[i].coef_[0]\n",
    "\n",
    "        top_positive = np.argsort(class_coefficients)[-20:][::-1]         \n",
    "\n",
    "        feature_names = tfidf_vect.get_feature_names_out()\n",
    "        \n",
    "        top_positive_words = []\n",
    "        for i in top_positive:\n",
    "            top_positive_words.append(str_vect.get_feature_names_out()[i])\n",
    "        \n",
    "        list_of_pos_correlations.append(top_positive_words)\n",
    "\n",
    "    nl = list(list_of_pos_correlations)\n",
    "    df_top = pd.DataFrame(nl)\n",
    "    df_top = df_top.transpose()\n",
    "    df_top.columns = le_group.classes_\n",
    "    return df_top, class_report, f1score, class_reportfll\n",
    "\n",
    "\n",
    "\n",
    "#Read in translated dataset df_full\n",
    "df_full = pd.read_csv(\"df_translated_proc_7thnov.csv\", dtype = object)\n",
    "#Read in annotated dataset df_annotation\n",
    "df_annotation = pd.read_csv(\"annotated_data/annotations_noVil_2024_11_04_13_35_81ae6912.csv\", dtype=object)\n",
    "\n",
    "df_full.loc[:, \"merged_text\"] = (df_full[\"Translated_descriptions_pro\"].fillna('') + \n",
    "                                 df_full[\"Translated_title_pro\"].fillna('') + \n",
    "                                 df_full[\"Translated_receiver_title_pro\"].fillna(''))\n",
    "\n",
    "#Dropping columns we don't need\n",
    "df_full = df_full.drop(columns = [\"Translated_descriptions_pro\", \"Translated_title_pro\", \n",
    "                                  \"Translated_receiver_title_pro\", \"Unnamed: 0\"])\n",
    "df_annotation = df_annotation.drop(columns = [\"Unnamed: 0\", 'Unnamed: 1', \"annotation_id\", \n",
    "                                              \"annotator\", \"created_at\", \"id\", \"lead_time\", \"updated_at\"])\n",
    "\n",
    "#Description lower convertion\n",
    "df_annotation[\"Description\"] = df_annotation[\"Description\"].str.lower()\n",
    "\n",
    "#Removing Villum from annotation mask as the data was bugged\n",
    "anno_novilmask = df_annotation[\"Foundation_name\"]== \"VILLUM\"  \n",
    "df_annotation_novil = df_annotation[~anno_novilmask]  \n",
    "\n",
    "\n",
    "print(\"shape of full dataframe before merge :\", df_full.shape)\n",
    "#print(\"Columns in full dataset :\", df_full.columns)\n",
    "print(\"shape of annotated dataframe before merge :\", df_annotation_novil.shape)\n",
    "#print(\"Columns in annotated dataset :\", df_annotation.columns)\n",
    "\n",
    "\n",
    "### Next step is to make annotation data from gives categories \n",
    "\n",
    "#MERGE\n",
    "\n",
    "merge_listnames = [\"Description\", \"Grant_size_(DKK)\", \"Foundation_name\", \"Year\", \"Receiver\",\n",
    "        \"Receiver_Title\", \"Title\", \"Country\", \"Institution\", \"Project_Category\", \n",
    "        \"Project_Subcategory\", \"Receiver_Name\", \"Receiver_Profession\", \"Region\"]\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    df_annotation_novil,\n",
    "    df_full,  \n",
    "    on=merge_listnames, \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop_duplicates(subset= merge_listnames)\n",
    "\n",
    "df_remains = pd.merge(\n",
    "    df_full,\n",
    "    merged_df,\n",
    "    on=merge_listnames,\n",
    "    how=\"left\",\n",
    "    indicator=True)\n",
    "\n",
    "df_remains = df_remains[df_remains[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n",
    "\n",
    "df_remains=df_remains.drop(columns=['merged_text_y', 'Translated_descriptions_y', 'Translated_receiver_title_y', \n",
    "                                    'Translated_title_y', \"sentiment\"])\n",
    "\n",
    "df_remains = df_remains.rename(columns = {\"Translated_title_x\" : \"Translated_title\", \n",
    "                                          \"Translated_receiver_title_x\" : \"Translated_receiver_title\",\n",
    "                             \"Translated_descriptions_x\" : \"Translated_descriptions\", \n",
    "                                          \"merged_text_x\" : \"merged_text\"})\n",
    "\n",
    "\n",
    "print(\"shape of full dataset (-annotations) after merge :\", df_remains.shape)\n",
    "\n",
    "print(\"shape of annotated dataframe after merge :\", merged_df.shape)\n",
    "\n",
    "\n",
    "print(\"Five lines where removed as they were either changed after remaking the dataset, or actual duplicates\",\n",
    "      \"- but they were removed to make sure the data is consistent\")\n",
    "\n",
    "#Data processing the remaining lines \n",
    "\n",
    "list_of_columns = [\"Receiver\", \"Project_Subcategory\", \"Project_Category\", \"Title\", \"Institution\", \"Region\", \n",
    "                   \"Receiver_Name\", \"Receiver_Profession\"]\n",
    "\n",
    "for i in list_of_columns:\n",
    "    df_remains[i] = data_processing_function(df_remains[i].dropna())\n",
    "    merged_df[i] = data_processing_function(merged_df[i].dropna())\n",
    "\n",
    "\n",
    "\n",
    "no_cat = ~df_remains[\"Project_Category\"].isna()  #Columns that have Project_Category\n",
    "no_sub_cat = df_remains[\"Project_Subcategory\"].isna() #Columns that don't have Project_SUBcategory\n",
    "df_cat_and_no_sub_cat = df_remains.loc[no_cat & no_sub_cat]\n",
    "df_subcats = df_remains.loc[~no_sub_cat]\n",
    "df_cats = df_cat_and_no_sub_cat.groupby([\"Project_Category\"])[\"Year\"].count()\n",
    "df_subcategories = df_subcats.groupby([\"Project_Subcategory\"])[\"Year\"].count()\n",
    "#print(df_cats) #print to see how i chose categories\n",
    "#print(df_subcategories) #print to see how i chose categories\n",
    "#print(\"Dataframe Subcategories :\", df_subcategories)\n",
    "\n",
    "Education_and_public_information = [\"diversitet  didaktik og uddannelse\", \"science  fritiden\", \n",
    "        \"undersøgende virkelighedsnær naturfagsundervisning\", \"viden og uddannelse\", \"viden uddannelse\"]\n",
    "\n",
    "Culture = [\"kunst og kultur\"]\n",
    "\n",
    "Nature_Preservation_and_environment = [\"miljø og klima\"]\n",
    "\n",
    "Social_Purpose = [\"sociale indsatser\"]\n",
    "\n",
    "Research = [\"teknisk og naturvidenskabelig forskning\", \"villum experiment\", \"villum international postdoc\", \n",
    "            \"villum investigator\", \"villum kann rasmussen professorat\", \"villum kann rasmussens årslegat\", \n",
    "            \"villum synergy\", \"villum young investigator\"]\n",
    "\n",
    "subcategories_dict = {\n",
    "    \"Education_and_public_information\": Education_and_public_information,\n",
    "    \"Culture\": Culture,\n",
    "    \"Nature_Preservation_and_environment\": Nature_Preservation_and_environment,\n",
    "    \"Social_Purpose\": Social_Purpose,\n",
    "    \"Research_and_Science\": Research,\n",
    "}\n",
    "\n",
    "\n",
    "#Choosing these categories as label info in Project_Category : \n",
    "\n",
    "Research = [\"brain prize\", \"ascending investigators\", \"earlycareer clinician scientists\", \n",
    "            \"education  awareness grants\", \"conferences\", \"experiment\", \"fellowship\", \n",
    "            \"field trips research stays 100000\", \"forskning\", \"forskning og innovation\", \n",
    "            \"forskning og læring\", \"frontier grant\", \"grants  excellence\", \n",
    "            \"international neuroscience programme\", \"international postdocs\", \n",
    "            \"internationalisation fellowships\", \"internationalisation programmes\", \n",
    "            \"larger biomedical projects\", \"larger international meetings  conferences\", \n",
    "            \"leo foundation awards\", \"leo foundation dr abildgaard fellowships\", \n",
    "            \"lf nih brain initiative\", \"lfin investigator network\", \"monograph fellowships\", \n",
    "            \"nordic research prize\", \"phd scholarships\", \"postdocs\", \n",
    "            \"postdoctoral fellowship   danish institute  athens\", \"pregraduate scholarships\", \n",
    "            \"professorships\", \"reintegration fellowships\", \"research grants  open competition\", \n",
    "            \"research infrastructure\", \"research learning active ownership\", \"research networking\", \n",
    "            \"semper ardens accelerate\", \"semper ardens accomplish\", \"semper ardens advance\", \n",
    "            \"serendipity grants\", \"visiting fellowships  university  oxford\", \"visiting professorships\", \n",
    "            \"young investigator prize\"]\n",
    "\n",
    "Education_and_public_information = [\"junior brain prize\", \"alle børn skal kunne læse og regne\", \n",
    "            \"børn og unge parat til uddannelse\", \"godt på vej  ungdomsuddannelse\", \n",
    "            \"sec science education communication\", \"science communication\", \n",
    "            \"stærke rammer  tryghed styrk civilsamfund debat og demokrati\", \"uddannelse\"]\n",
    "\n",
    "Culture = [\"aktuel kunst\", \"klassisk musik\", \"kunst og kultur\", \"publication\", \"samtidskunst\"]\n",
    "\n",
    "Health = [\"sikkerhed\", \"sikkerhed akut hjælp\", \"sikkerhed forebyg brand\", \"sikkerhed respekt  vand\", \n",
    "          \"sikkerhed sikker  trafikken\", \"skadeforebyggelse\", \"skadeforebyggelse medlemmer\", \n",
    "          \"mere ungdom mindre sygdom\", \"sundhed\", \"sundhed akut hjælp\", \"sundhed lev med kronisk sygdom\", \n",
    "          \"sundhed lev sundt\", \"sundhed mental sundhed\", \"sundhed patienten først\", \n",
    "          \"tryghed  hverdagen hjertestart\", \"tryghed  hverdagen reager på stroke\", \n",
    "          \"tryghed  hverdagen respekt  vand\", \"unges mentale sundhed\", \"voksne sundere liv\"]\n",
    "\n",
    "Daughter_Foundation = [\"bikubenfonden\", \"velux fonden\"]\n",
    "\n",
    "Social = [\"børn og unge alle med fra start\", \"et bedre liv som anbragt\", \"social responsibility inclusion\", \n",
    "          \"sociale indsatser\", \"lige muligheder  børn  fattigdom\", \"sport\", \"trivsel\", \n",
    "          \"trivsel en chance  livet\", \"trivsel en chance  livet\", \"trivsel en plads  fællesskabet\", \n",
    "          \"trivsel et liv uden kriminalitet\", \"trivsel mental sundhed\", \"unge på kanten\", \n",
    "          \"ældre et godt liv som pårørende\", \"voksne fællesskaber  alle\"]\n",
    "\n",
    "International_Humanitarian = [\"safe water\"]\n",
    "\n",
    "Nature_Preservation_and_environment = [\"et hav  balance\", \"svanninge bjerge\", \"vand og bæredygtig udvikling\"]\n",
    "\n",
    "Religion = [\"folkekirken\"]\n",
    "\n",
    "categories_dict = {\n",
    "    \"Research_and_Science\": Research,\n",
    "    \"Education_and_public_information\": Education_and_public_information,\n",
    "    \"Culture\": Culture,\n",
    "    \"Health\": Health,\n",
    "    \"Daughter_Foundation\": Daughter_Foundation,\n",
    "    \"Social_Purpose\": Social,\n",
    "    \"International_Humanitarian\": International_Humanitarian,\n",
    "    \"Nature_Preservation_and_environment\": Nature_Preservation_and_environment,\n",
    "    \"Religion\": Religion\n",
    "    \n",
    "}\n",
    "#categories_dict\n",
    "\n",
    "#Running the find_category function defined above\n",
    "annotation_subcat = find_category(df_remains[\"Project_Subcategory\"], subcategories_dict)\n",
    "annotation_cat = find_category(df_remains[\"Project_Category\"], categories_dict)\n",
    "\n",
    "#using list comprehension to insert sentinent in output list, category is inserted if subcategory is not there, if none = np.nan\n",
    "output_list = [x if x != \"NA\" else y for x, y in zip(annotation_subcat, annotation_cat)]\n",
    "df_remains[\"sentiment\"]=output_list\n",
    "\n",
    "\n",
    "mask_sentiment = df_remains[\"sentiment\"] == \"NA\"\n",
    "new_labeldf = df_remains[~mask_sentiment]\n",
    "label_remains = df_remains[mask_sentiment]\n",
    "label_remains.drop(columns = [\"sentiment\"])\n",
    "\n",
    "print(\"Are length of new_labeldf + label_remains equal to df_remains? :\", (len(new_labeldf)+len(label_remains))==len(df_remains))\n",
    "\n",
    "print(\"\\n\")\n",
    "#making sure its all strings\n",
    "all_strings = merged_df[\"merged_text\"].apply(lambda x: isinstance(x, str)).all()\n",
    "if all_strings:\n",
    "    print(\"All rows in the column are strings.\")\n",
    "else:\n",
    "    print(\"Not all rows in the column are strings.\")\n",
    "\n",
    "merged_df['sentiment'] = merged_df['sentiment'].str.replace(\" \", \"_\")\n",
    "merged_df['sentiment'] = merged_df['sentiment'].str.replace(\"-\", \"_\")\n",
    "merged_df= merged_df.dropna(subset=['sentiment'])\n",
    "\n",
    "new_labeldf['sentiment'] = new_labeldf['sentiment'].str.replace(\" \", \"_\")\n",
    "new_labeldf['sentiment'] = new_labeldf['sentiment'].str.replace(\"-\", \"_\")\n",
    "new_labeldf= new_labeldf.dropna(subset=['sentiment'])\n",
    "\n",
    "\n",
    "#extra_label_df = merged_df.concat(new_labeldf)\n",
    "\n",
    "extra_label_df = pd.concat([merged_df, new_labeldf])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"self annotated dataframe :\", merged_df.shape)\n",
    "print(\"\\n\")\n",
    "print(\"remains from self annotated dataframe :\", df_remains.shape)\n",
    "print(\"\\n\")\n",
    "print(\"self annotated dataframe + lables from selected project categories :\", extra_label_df.shape)\n",
    "print(\"\\n\")\n",
    "print(\"remains from self annotated dataframe + lables from selected project categories :\", label_remains.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Are the dataset lengths coherent? :\", len(df_remains)+len(merged_df)==len(extra_label_df)+len(label_remains))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"tf_idf values for self annotated df :\")\n",
    "tf_idf_fun(merged_df[\"merged_text\"])\n",
    "print(\"tf_idf values for self annotated df + added labels from categories:\")\n",
    "tf_idf_fun(extra_label_df[\"merged_text\"])\n",
    "\n",
    "\n",
    "abbreviations = {\n",
    "    \"Business__and_regional_development\": \"Bus._&_reg._dev.\",\n",
    "    \"Daughter_Foundation\": \"Daughter_found.\",\n",
    "    \"Education_and_public_information\": \"Edu_.&_public_info.\",\n",
    "    \"International_Other\": \"Int._other\",\n",
    "    \"International_Humanitarian\": \"Int._humanitarian\",\n",
    "    \"Nature_Preservation_and_environment\": \"Nature_pres._&_env.\",\n",
    "    \"Research_and_Science\": \"Research\"\n",
    "}\n",
    "\n",
    "merged_df[\"sentiment\"] = merged_df[\"sentiment\"].replace(abbreviations)\n",
    "extra_label_df[\"sentiment\"] = extra_label_df[\"sentiment\"].replace(abbreviations)\n",
    "\n",
    "make_distribution(merged_df, \"Self-labeled data\")\n",
    "\n",
    "make_distribution(extra_label_df, \"Labels from source + Self-labeled data\")\n",
    "\n",
    "\n",
    "class_w = ca_class_weights(merged_df[\"sentiment\"])\n",
    "print(\"Class weights merged_df:\", class_w)\n",
    "print(\"\\n\")\n",
    "class_w_extra = ca_class_weights(extra_label_df[\"sentiment\"])\n",
    "print(\"Class weights extra labels:\", class_w_extra)\n",
    "\n",
    "\n",
    "#Part 3 ----- Prediction\n",
    "\n",
    "\n",
    "\n",
    "print(\"self annotated dataframe :\", merged_df.shape)\n",
    "\n",
    "print(\"remains from self annotated dataframe :\", df_remains.shape)\n",
    "\n",
    "print(\"self annotated dataframe + lables from selected project categories :\", extra_label_df.shape)  #this\n",
    "\n",
    "print(\"remains from self annotated dataframe + lables from selected project categories :\", label_remains.shape)  #and this\n",
    "\n",
    "\n",
    "\n",
    "#shortcut_versions! 1\n",
    "\n",
    "#test1\n",
    "test1_df_new_pred, test1_top  = all_reports_cv(merged_df, df_remains, \"self_annotated\")\n",
    "#test2\n",
    "test2_df_new_pred, test2_top  = all_reports_cv(extra_label_df, label_remains, \"TEST2_Extended_Label_data\")\n",
    "\n",
    "\n",
    "# again but with added info to remaining test 3-5\n",
    "\n",
    "\n",
    "df_extra_label_df_v1 = extra_label_df.copy()\n",
    "df_extra_label_df_v2 = extra_label_df.copy()\n",
    "df_extra_label_df_v3 = extra_label_df.copy()\n",
    "label_remains_v1 = label_remains.copy()\n",
    "label_remains_v2 = label_remains.copy()\n",
    "label_remains_v3 = label_remains.copy()\n",
    "\n",
    "abbreviations = {\n",
    "    \"Business__and_regional_development\": \"Bus._&_reg._dev.\",\n",
    "    \"Daughter_Foundation\": \"Daughter_found.\",\n",
    "    \"Education_and_public_information\": \"Edu_.&_public_info.\",\n",
    "    \"International_Other\": \"Int._other\",\n",
    "    \"International_Humanitarian\": \"Int._humanitarian\",\n",
    "    \"Nature_Preservation_and_environment\": \"Nature_pres._&_env.\",\n",
    "    \"Research_and_Science\": \"Research\"\n",
    "}\n",
    "\n",
    "extra_label_df[\"sentiment\"] = extra_label_df[\"sentiment\"].replace(abbreviations)\n",
    "\n",
    "df_extra_label_df_v1[\"merged_text\"] = df_extra_label_df_v1[\"merged_text\"].fillna('') + \" \" + df_extra_label_df_v1[\"Receiver\"].fillna('') +  \" \" + df_extra_label_df_v1[\"Institution\"].fillna('') +  \" \" + df_extra_label_df_v1[\"Receiver_Profession\"].fillna('') +  \" \" + df_extra_label_df_v1[\"Country\"].fillna('')\n",
    "label_remains_v1[\"merged_text\"] = label_remains_v1[\"merged_text\"].fillna('') + \" \" + label_remains_v1[\"Receiver\"].fillna('') +  \" \" + label_remains_v1[\"Institution\"].fillna('') +  \" \" + label_remains_v1[\"Receiver_Profession\"].fillna('') +  \" \" + label_remains_v1[\"Country\"].fillna('')\n",
    "\n",
    "\n",
    "df_extra_label_df_v2[\"merged_text\"] = df_extra_label_df_v2[\"merged_text\"].fillna('') + \" \" + df_extra_label_df_v2[\"Receiver\"].fillna('') + \" \" + df_extra_label_df_v2[\"Institution\"].fillna('') + \" \" + df_extra_label_df_v2[\"Receiver_Name\"].fillna('') + \" \" + df_extra_label_df_v2[\"Receiver_Profession\"].fillna('') + \" \" + df_extra_label_df_v2[\"Country\"].fillna('')\n",
    "label_remains_v2[\"merged_text\"] = label_remains_v2[\"merged_text\"].fillna('') + \" \" + label_remains_v2[\"Receiver\"].fillna('') + \" \" + label_remains_v2[\"Institution\"].fillna('') + \" \" + label_remains_v2[\"Receiver_Name\"].fillna('') + \" \" + label_remains_v2[\"Receiver_Profession\"].fillna('') + \" \" + label_remains_v2[\"Country\"].fillna('')\n",
    "\n",
    "\n",
    "df_extra_label_df_v3[\"merged_text\"] = df_extra_label_df_v3[\"merged_text\"].fillna('') + \" \" + df_extra_label_df_v3[\"Receiver\"].fillna('') + \" \" + df_extra_label_df_v3[\"Institution\"].fillna('')\n",
    "label_remains_v3[\"merged_text\"] = label_remains_v3[\"merged_text\"].fillna('') + \" \" + label_remains_v3[\"Receiver\"].fillna('') + \" \" + label_remains_v3[\"Institution\"].fillna('')\n",
    "\n",
    "test3_df_new_pred, test3_top  = all_reports_cv(df_extra_label_df_v1, label_remains_v1, \"TEST3_extended_version3_1\")\n",
    "test4_df_new_pred, test4_top  = all_reports_cv(df_extra_label_df_v2, label_remains_v2, \"TEST4_extended_version4_2\")\n",
    "test5_df_new_pred, test5_top  = all_reports_cv(df_extra_label_df_v3, label_remains_v3, \"TEST5_extended_version5_3\")\n",
    "\n",
    "#MAKING TEST 2 DF INPUT CONCAT\n",
    "\n",
    "\n",
    "df_test2_combined = pd.concat([label_remains, test2_df_new_pred])\n",
    "\n",
    "df_test2_combined[\"pred_merge\"] = df_test2_combined[\"prediction\"].fillna(df_test2_combined[\"sentiment\"])\n",
    "\n",
    "df_combined2 = df_test2_combined.copy()\n",
    "\n",
    "df_combined2['Grant_size_(DKK)'] = pd.to_numeric(df_combined2['Grant_size_(DKK)'], errors='coerce')\n",
    "df_combined2['Year'] = pd.to_numeric(df_combined2['Year'], errors='coerce')\n",
    "\n",
    "grouped_category2 = df_combined2.groupby('pred_merge').size()\n",
    "grouped_category_grantsize2 = df_combined2.groupby('pred_merge')['Grant_size_(DKK)'].sum()\n",
    "\n",
    "make_distribution2(grouped_category2, \"D_total (labeled + predicted) Categories\")\n",
    "make_distribution2(grouped_category_grantsize2, \"D_total - Grant Size in DKK\")\n",
    "\n",
    "\n",
    "df_fullLUNDBECK2, class_report_dictLUNDBECK2, f1score_LUNDBECK2, class_report_LUNDBECK2 = foundations_asso_print(df_combined2, \"LUNDBECK\")\n",
    "df_fullLUNDBECK2\n",
    "print(\"Classification Report : Lundbeck Foundation\")\n",
    "print(\"Weighted F-1 score :\", f1score_LUNDBECK2)\n",
    "print(class_report_LUNDBECK2)\n",
    "\n",
    "\n",
    "df_fullveluxtest2, class_report_dictveluxtest2, f1score_veluxtest2, class_report_veluxtest2 = foundations_asso_print(df_combined2, \"VELUX\")\n",
    "df_fullveluxtest2\n",
    "print(\"Classification Report : Velux Foundation\")\n",
    "print(\"Weighted F-1 score :\", f1score_veluxtest2)\n",
    "print(class_report_veluxtest2)\n",
    "\n",
    "\n",
    "df_full_novo_test2, class_report_dictnovo_test2, f1score_novo_test2, class_report_novo_test2 = foundations_asso_print(df_combined2, \"NOVO\")\n",
    "df_full_novo_test2\n",
    "\n",
    "print(\"Classification Report : Novo Nordisk Foundation\")\n",
    "print(\"Weighted F-1 score :\", f1score_novo_test2)\n",
    "print(class_report_novo_test2)\n",
    "\n",
    "df_full_carls2, class_report_dictcarls2, f1score_carls2, class_report_carls2 = foundations_asso_print(df_combined2, \"CARLSBERG\")\n",
    "print(\"Classification Report : Carlsberg Foundation\")\n",
    "print(\"Weighted F-1 score :\", f1score_carls2)\n",
    "print(class_report_carls2)\n",
    "df_full_carls2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
